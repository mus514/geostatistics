{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding unlabeled audio for bird classification using Perch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a coding environment for Perch, a bird clkassification model developed by google\n",
    "\n",
    "https://github.com/google-research/perch  \n",
    "https://www.kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/4  \n",
    "https://www.kaggle.com/code/tabassumnova/google-bird-vocalization-classifier-model-77-acc/notebook\n",
    "\n",
    "To create the environment for this workflow follow the following steps:\n",
    "1) Install Anaconda (https://www.anaconda.com/download/)\n",
    "2) Create and activate environment, using powershell\n",
    "```\n",
    "conda create --name GooglePerch python=3.10\n",
    "conda activate GooglePerch\n",
    "```\n",
    "\n",
    "3) Install base libraries\n",
    "```\n",
    "conda install -c conda-forge mamba\n",
    "mamba install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0 cudatoolkit-dev ipykernel ipython nbformat numpy pandas  cudatoolkit-dev matplotlib glob2\n",
    "python -m pip install \"tensorflow<2.11\" \"tensorflow-gpu<2.11\" \"tensorflow-io==0.27.0\" tensorflow-hub librosa\n",
    "```\n",
    "\n",
    "4) Check tensorflow install\n",
    "```\n",
    "python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "```\n",
    "\n",
    "https://stackoverflow.com/questions/68614547/tensorflow-libdevice-not-found-why-is-it-not-found-in-the-searched-path\n",
    "\n",
    "5) Remove and clean\n",
    "```\n",
    "conda deactivate\n",
    "conda remove --name GooglePerch --all\n",
    "conda build purge\n",
    "conda clean -a -i -p -t -f -c -l\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "# Set memory growing to true\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "      for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "# Hide warning messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Disable eager execution \n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Data loading and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File navigation\n",
    "#import glob\n",
    "#from pathlib import Path\n",
    "#import io\n",
    "import os\n",
    "\n",
    "# Viewing\n",
    "#from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model and lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory\n",
    "dir = \"P:\\\\Projets\\\\Actif\\\\2023_ECCC4_Biodiv\\\\3-Analyses\\\\\"\n",
    "\n",
    "# Load model and lables locally. We are using version 4\n",
    "model = hub.load(dir + \"2-Analyses\\\\Perch\\\\\")\n",
    "labels_path = hub.resolve(dir + \"2-Analyses\\\\Perch\\\\\") + \"assets\\\\label.csv\"\n",
    "\n",
    "# From the hub\n",
    "#model = hub.load('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/4')\n",
    "#labels_path = hub.resolve('https://kaggle.com/models/google/bird-vocalization-classifier/frameworks/tensorFlow2/variations/bird-vocalization-classifier/versions/4') + \"/assets/label.csv\"\n",
    "\n",
    "# Grab list of birds the model has trained on\n",
    "def class_names_from_csv(labels_path):\n",
    "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "    with open(labels_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        class_names = [row[0] for row in csv_reader]\n",
    "        return class_names[1:]\n",
    "model_classes=class_names_from_csv(labels_path)\n",
    "\n",
    "# Save to disk?\n",
    "#model_classesDF = pd.DataFrame(model_classes)\n",
    "#model_classesDF.to_csv(dir + \"1-Data\\\\Biodiversity\\\\Perchm_model_classes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taxonomy data for filtering species codes\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomy:\n",
    "    taxonomy = json.load(taxonomy)\n",
    "taxonomy = taxonomy.get(\"mappings\", None)\n",
    "taxonomy = taxonomy.get(\"ebird2021_clements_to_species\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra:\n",
    "    taxonomyextra = json.load(taxonomyextra)\n",
    "taxonomyextra = taxonomyextra.get(\"mappings\", None)\n",
    "taxonomyextra = taxonomyextra.get(\"ebird2022_clements_to_species\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra2:\n",
    "    taxonomyextra2 = json.load(taxonomyextra2)\n",
    "taxonomyextra2 = taxonomyextra2.get(\"mappings\", None)\n",
    "taxonomyextra2 = taxonomyextra2.get(\"ioc_12_2_to_ebird2021\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra3:\n",
    "    taxonomyextra3 = json.load(taxonomyextra3)\n",
    "taxonomyextra3 = taxonomyextra3.get(\"mappings\", None)\n",
    "taxonomyextra3 = taxonomyextra3.get(\"xenocanto_11_2_to_ebird2022_species\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra4:\n",
    "    taxonomyextra4 = json.load(taxonomyextra4)\n",
    "taxonomyextra4 = taxonomyextra4.get(\"mappings\", None)\n",
    "taxonomyextra4 = taxonomyextra4.get(\"ebird2022_to_species\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra5:\n",
    "    taxonomyextra5 = json.load(taxonomyextra5)\n",
    "taxonomyextra5 = taxonomyextra5.get(\"mappings\", None)\n",
    "taxonomyextra5 = taxonomyextra5.get(\"xenocanto_to_ebird2021\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra6:\n",
    "    taxonomyextra6 = json.load(taxonomyextra6)\n",
    "taxonomyextra6 = taxonomyextra6.get(\"mappings\", None)\n",
    "taxonomyextra6 = taxonomyextra6.get(\"ebird2021_to_genus\", None)\n",
    "\n",
    "with open(dir + \"2-Analyses\\\\Perch\\\\assets\\\\taxonomy_database.json\", \"r\") as taxonomyextra7:\n",
    "    taxonomyextra7 = json.load(taxonomyextra7)\n",
    "taxonomyextra7 = taxonomyextra7.get(\"mappings\", None)\n",
    "taxonomyextra7 = taxonomyextra7.get(\"ebird2022_to_genus\", None)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "taxonomy = pd.DataFrame(taxonomy)\n",
    "taxonomy.reset_index(inplace=True)\n",
    "taxonomy = taxonomy.iloc[:, :2]\n",
    "taxonomy.rename(columns={\"index\": \"Sp_name\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra = pd.DataFrame(taxonomyextra)\n",
    "taxonomyextra.reset_index(inplace=True)\n",
    "taxonomyextra = taxonomyextra.iloc[:, :2]\n",
    "taxonomyextra.rename(columns={\"index\": \"Sp_name\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra2 = pd.DataFrame(taxonomyextra2)\n",
    "taxonomyextra2.reset_index(inplace=True)\n",
    "taxonomyextra2 = taxonomyextra2.iloc[:, :2]\n",
    "taxonomyextra2.rename(columns={\"index\": \"Sp_name\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra3 = pd.DataFrame(taxonomyextra3)\n",
    "taxonomyextra3.reset_index(inplace=True)\n",
    "taxonomyextra3 = taxonomyextra3.iloc[:, :2]\n",
    "taxonomyextra3.rename(columns={\"index\": \"Sp_name\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra4 = pd.DataFrame(taxonomyextra4)\n",
    "taxonomyextra4.reset_index(inplace=True)\n",
    "taxonomyextra4 = taxonomyextra4.iloc[:, :2]\n",
    "taxonomyextra4.rename(columns={\"index\": \"Incorrect_code\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra5 = pd.DataFrame(taxonomyextra5)\n",
    "taxonomyextra5.reset_index(inplace=True)\n",
    "taxonomyextra5 = taxonomyextra5.iloc[:, :2]\n",
    "taxonomyextra5.rename(columns={\"index\": \"Sp_name\", \"mapped_pairs\": \"Sp_code\"}, inplace=True)\n",
    "\n",
    "taxonomyextra6 = pd.DataFrame(taxonomyextra6)\n",
    "taxonomyextra6.reset_index(inplace=True)\n",
    "taxonomyextra6 = taxonomyextra6.iloc[:, :2]\n",
    "taxonomyextra6.rename(columns={\"index\": \"Sp_code\", \"mapped_pairs\": \"Genus_name\"}, inplace=True)\n",
    "\n",
    "taxonomyextra7 = pd.DataFrame(taxonomyextra7)\n",
    "taxonomyextra7.reset_index(inplace=True)\n",
    "taxonomyextra7 = taxonomyextra7.iloc[:, :2]\n",
    "taxonomyextra7.rename(columns={\"index\": \"Sp_code\", \"mapped_pairs\": \"Genus_name\"}, inplace=True)\n",
    "\n",
    "# Merge data \n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name_x','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Sp_name_y': 'Sp_name'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra2, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name_x','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Sp_name_y': 'Sp_name'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra3, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name_x','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Sp_name_y': 'Sp_name'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra5, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name_x','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Sp_name_y': 'Sp_name'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra6, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name','_merge'], inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra7, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'right_only']\n",
    "taxonomy_merged.drop(columns=['Sp_name','Genus_name_x','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Genus_name_y': 'Genus_name'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "taxonomy_merged = pd.merge(taxonomy,taxonomyextra4, on='Sp_code', how='outer', indicator=True)\n",
    "taxonomy_merged = taxonomy_merged[taxonomy_merged['_merge'] == 'both']\n",
    "taxonomy_merged.drop(columns=['Sp_code','_merge'], inplace=True)\n",
    "taxonomy_merged.rename(columns={'Incorrect_code': 'Sp_code'}, inplace=True)\n",
    "taxonomy = pd.concat([taxonomy, taxonomy_merged], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "taxonomy = taxonomy.drop_duplicates(subset='Sp_code', keep='first')\n",
    "\n",
    "# Save to disk?\n",
    "#taxonomy.to_csv(dir + \"1-Data\\\\Biodiversity\\\\taxonomy.csv\", index=False)\n",
    "\n",
    "# Clean environment\n",
    "del taxonomyextra, taxonomyextra2, taxonomyextra3, taxonomyextra4\n",
    "del taxonomyextra5, taxonomyextra6, taxonomyextra7, taxonomy_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nesting birds of Quebec\n",
    "nestingBirds=pd.read_csv(dir + \"1-Data\\\\Biodiversity\\\\Nesting_Birds_Quebec.csv\")\n",
    "classes = sorted(nestingBirds['Sp_code'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are birds we are interested in, that the model has not been trained on\n",
    "forced_defaults = 0\n",
    "class_map = []\n",
    "for c in classes:\n",
    "    try:\n",
    "        i = classes.index(c)\n",
    "        class_map.append(i)\n",
    "    except:\n",
    "        class_map.append(0)\n",
    "        forced_defaults += 1\n",
    "if forced_defaults > 0:\n",
    "    print(f\"There are {forced_defaults} birds Perch has not been trained on.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean environment\n",
    "del c, i, nestingBirds, forced_defaults, gpus, gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process the data for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Ensure audio is 5s long and in correct dimention\n",
    "def frame_audio(\n",
    "      audio_array: np.ndarray,\n",
    "      window_size_s: float = 5.0,\n",
    "      hop_size_s: float = 5.0,\n",
    "      sample_rate = 32000,\n",
    "      ) -> np.ndarray:    \n",
    "    \"\"\"Helper function for framing audio for inference\"\"\"\n",
    "    if window_size_s is None or window_size_s < 0:\n",
    "        return audio_array[np.newaxis, :]\n",
    "    frame_length = int(window_size_s * sample_rate)\n",
    "    hop_length = int(hop_size_s * sample_rate)\n",
    "    framed_audio = tf.signal.frame(audio_array, frame_length, hop_length, pad_end=True)\n",
    "    return framed_audio\n",
    "\n",
    "# Ensure audio is at 32000 sampling rate\n",
    "def ensure_sample_rate(\n",
    "        waveform,\n",
    "        original_sample_rate,\n",
    "        desired_sample_rate=32000):\n",
    "    \"\"\"Resample waveform if required\"\"\"\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        waveform = tfio.audio.resample(waveform, original_sample_rate, desired_sample_rate)\n",
    "    return desired_sample_rate, waveform\n",
    "\n",
    "# Predict on every 5s increment of audio file and return a dataframe with predictions\n",
    "def predictions(\n",
    "        file_path,          # Path to audio file\n",
    "        message = False):    # Boolean to indicate printing progress messages\n",
    "    \"\"\"Loading audio 5s at a time for inference\"\"\"\n",
    "\n",
    "    # Print message\n",
    "    if message == True:\n",
    "        print(f\"Processing {file_path}\")\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # Get the duration of the audio in seconds\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "    # Define the length of each increment (in seconds)\n",
    "    increment_length = 5\n",
    "\n",
    "    # Calculate the number of increments (including padding)\n",
    "    num_increments = int(np.ceil(duration / increment_length))\n",
    "\n",
    "    # Calculate the target duration after padding\n",
    "    padded_duration = num_increments * increment_length\n",
    "\n",
    "    # Pad the audio if necessary\n",
    "    if padded_duration > duration:\n",
    "        pad_length = int((padded_duration - duration) * sr)\n",
    "        y = np.pad(y, (0, pad_length), mode='constant')\n",
    "\n",
    "    # Update the duration after padding\n",
    "    newduration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "    # Create lists for storing results\n",
    "    increment_numbers = []\n",
    "    sp_codes = []\n",
    "    sp_names = []\n",
    "    probs = []\n",
    "    file_names = []\n",
    "    file_paths = []\n",
    "    recording_durations = []\n",
    "    number_5s_increments = []    \n",
    "\n",
    "    # Loop over each increment\n",
    "    for i in range(num_increments):\n",
    "        # Calculate start and end time for the current increment\n",
    "        start_time = i * increment_length\n",
    "        end_time = min((i + 1) * increment_length, newduration)\n",
    "        \n",
    "        # Extract the audio for the current increment\n",
    "        increment_audio = y[int(start_time * sr):int(end_time * sr)]\n",
    "\n",
    "        # Prepare audio\n",
    "        _ , increment_audio = ensure_sample_rate(increment_audio, sr)\n",
    "        fixed_tm = frame_audio(increment_audio)\n",
    "\n",
    "        # Make test predictions\n",
    "        logits, _ = model.infer_tf(fixed_tm[:1])\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        argmax = np.argmax(probabilities)\n",
    "        SP_Code = model_classes[argmax]\n",
    "        SP_Name = taxonomy[taxonomy['Sp_code'] == SP_Code]['Sp_name'].iloc[0]\n",
    "        #if math.isnan(SP_Name):\n",
    "        #    SP_Name = taxonomy[taxonomy['Sp_code'] == SP_Code]['Genus_name'].iloc[0]\n",
    "        if isinstance(SP_Name, float):\n",
    "            SP_Name = taxonomy[taxonomy['Sp_code'] == SP_Code]['Genus_name'].iloc[0]\n",
    "\n",
    "        # Append values to respective lists\n",
    "        increment_numbers.append(i + 1)\n",
    "        sp_codes.append(SP_Code)\n",
    "        sp_names.append(SP_Name)\n",
    "        probs.append(probabilities[0].numpy().tolist()[argmax])\n",
    "        file_names.append(os.path.splitext(os.path.basename(file_path))[0])\n",
    "        file_paths.append(file_path)\n",
    "        recording_durations.append(duration)\n",
    "        number_5s_increments.append(num_increments)\n",
    "\n",
    "        # Print message\n",
    "        if message == True:\n",
    "            print(f\"Processing increment {i+1}/{num_increments}\")\n",
    "            print(f\"The audio is from the species {SP_Name} with probability of {probabilities[0].numpy().tolist()[argmax]}\")\n",
    "\n",
    "    # Create a dictionary to hold column data\n",
    "    data = {\n",
    "        'file_name': file_names,\n",
    "        'file_path': file_paths,\n",
    "        'recording_duration': recording_durations,\n",
    "        'number_5s_increments': number_5s_increments,\n",
    "        'increment_number': increment_numbers,\n",
    "        'Sp_code': sp_codes,\n",
    "        'Sp_name': sp_names,\n",
    "        'Prob': probs\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return\n",
    "    return df\n",
    "\n",
    "# Loop over directory of audio and predict \n",
    "def batch_predictions(\n",
    "        root_dir,           # Path to directory containing folders and audio files\n",
    "        message = False):   # Boolean to indicate printing progress messages\n",
    "    \"\"\"Predicting on all audio files in directory\"\"\"\n",
    "    \n",
    "    # Create list for storing results\n",
    "    results_df = []\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in tqdm(filenames):\n",
    "\n",
    "            # Check if the file is an audio file\n",
    "            if filename.endswith('.mp3') or filename.endswith('.wav') or filename.endswith('.ogg') or filename.endswith('.WAV'):\n",
    "                            \n",
    "                # Get the full path of the audio file\n",
    "                audio_file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Process the audio file\n",
    "                df = predictions(audio_file_path,message=message)\n",
    "\n",
    "                # Add to list\n",
    "                results_df.append(df)\n",
    "            \n",
    "    # Concatinate results\n",
    "    combined_df = pd.concat(results_df, ignore_index=True)\n",
    "\n",
    "    # Return\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify root directory for all files\n",
    "#root_dir = dir + \"1-Data\\\\Audiomoths2023\"\n",
    "\n",
    "# Predict and convert to data frame\n",
    "#df = batch_predictions(root_dir,False)\n",
    "#df.to_csv(dir + \"1-Data/Audiomoths2023_Processed/Perch_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Specify root directory\n",
    "#root_dir = dir + \"1-Data\\\\Audiomoths2023\\\\BOU-2023-006\"\n",
    "test_dir = \"C:\\\\Users\\\\Jurie\\\\Desktop\\\\Perch_TEST\\\\Audio_data\\\\New_folder\"\n",
    "\n",
    "# Preduct and convert to data frame\n",
    "df = batch_predictions(test_dir,False)\n",
    "\n",
    "# Post processing on dataframe\n",
    "dff = df.query('Prob > 0.5')\n",
    "#dff.to_csv(dir + \"1-Data/Audiomoths2023_Processed/Perch_output_All_TEST.csv\", index=False)\n",
    "#dff = dff.drop_duplicates(subset='Sp_code', keep='first')\n",
    "#dff.to_csv(dir + \"1-Data/Audiomoths2023_Processed/Perch_output_Unique_TEST.csv\", index=False)\n",
    "\n",
    "# Time = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv(\"C:\\\\Users\\\\Jurie\\\\Desktop\\\\Perch_TEST\\\\Perch_predictions_noise3.csv\", index=False)\n",
    "dff = dff.drop_duplicates(subset='Sp_code', keep='first')\n",
    "dff.to_csv(\"C:\\\\Users\\\\Jurie\\\\Desktop\\\\Perch_TEST\\\\Perch_predictions_Unique_noise3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to BirdNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdnet_df = pd.read_csv(\"C:\\\\Users\\\\Jurie\\\\Desktop\\\\Perch_TEST\\\\Birdnet_predictions_noise2.txt\",sep='\\t')\n",
    "birdnet_df = birdnet_df.query('Confidence > 0.5')\n",
    "birdnet_df = birdnet_df.drop_duplicates(subset='Species Code', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "perch_df =  pd.read_csv(\"C:\\\\Users\\\\Jurie\\\\Desktop\\\\Perch_TEST\\\\Perch_predictions_noise3.csv\")\n",
    "perch_df = perch_df.query('Prob > 0.5')\n",
    "perch_df = perch_df.drop_duplicates(subset='Sp_code', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing P:\\Projets\\Actif\\2023_ECCC4_Biodiv\\3-Analyses\\2-Analyses\\Perch\\Test_Data\\Canada_Goose_Example.wav\n",
      "Processing increment 1/4\n",
      "The audio is from the species branta canadensis with probability of 0.9973734617233276\n",
      "Processing increment 2/4\n",
      "The audio is from the species branta canadensis with probability of 0.9910955429077148\n",
      "Processing increment 3/4\n",
      "The audio is from the species branta canadensis with probability of 0.9837427735328674\n",
      "Processing increment 4/4\n",
      "The audio is from the species branta canadensis with probability of 0.9992631077766418\n"
     ]
    }
   ],
   "source": [
    "# TEST single prediction\n",
    "file_path = r\"P:\\Projets\\Actif\\2023_ECCC4_Biodiv\\3-Analyses\\2-Analyses\\Perch\\Test_Data\\Canada_Goose_Example.wav\"\n",
    "df = predictions(file_path,message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# TEST batch predictions\n",
    "root_dir = r\"P:\\Projets\\Actif\\2023_ECCC4_Biodiv\\3-Analyses\\2-Analyses\\Perch\\Test_Data\"\n",
    "df = batch_predictions(root_dir,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVELOPMENT TESTING\n",
    "file_path = r\"P:\\Projets\\Actif\\2023_ECCC4_Biodiv\\3-Analyses\\2-Analyses\\Perch\\Test_Data\\Canada_Goose_Example.wav\"\n",
    "# Load the audio file\n",
    "y, sr = librosa.load(file_path)\n",
    "\n",
    "# Get the duration of the audio in seconds\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "# Define the length of each increment (in seconds)\n",
    "increment_length = 5\n",
    "\n",
    "# Calculate the number of increments (including padding)\n",
    "num_increments = int(np.ceil(duration / increment_length))\n",
    "\n",
    "# Calculate the target duration after padding\n",
    "padded_duration = num_increments * increment_length\n",
    "\n",
    "# Pad the audio if necessary\n",
    "if padded_duration > duration:\n",
    "     pad_length = int((padded_duration - duration) * sr)\n",
    "     y = np.pad(y, (0, pad_length), mode='constant')\n",
    "\n",
    "# Update the duration after padding\n",
    "newduration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "# Create lists for storing results\n",
    "increment_numbers = []\n",
    "sp_codes = []\n",
    "sp_names = []\n",
    "probs = []\n",
    "file_names = []\n",
    "file_paths = []\n",
    "recording_durations = []\n",
    "number_5s_increments = []\n",
    "\n",
    "# Calculate start and end time for the current increment\n",
    "for i in range(num_increments):\n",
    "     # Calculate start and end time for the current increment\n",
    "     start_time = i * increment_length\n",
    "     end_time = min((i + 1) * increment_length, newduration)\n",
    "        \n",
    "     # Extract the audio for the current increment\n",
    "     increment_audio = y[int(start_time * sr):int(end_time * sr)]\n",
    "\n",
    "     # Prepare audio\n",
    "     _ , increment_audio = ensure_sample_rate(increment_audio, sr)\n",
    "     fixed_tm = frame_audio(increment_audio)\n",
    "\n",
    "     # Make test predictions\n",
    "     logits, _ = model.infer_tf(fixed_tm[:1])\n",
    "     probabilities = tf.nn.softmax(logits)\n",
    "     argmax = np.argmax(probabilities)\n",
    "     SP_Code = model_classes[argmax]\n",
    "     SP_Name = taxonomy[taxonomy['Sp_code'] == SP_Code]['Sp_name'].iloc[0]\n",
    "\n",
    "     # Append values to respective lists\n",
    "     increment_numbers.append(i + 1)\n",
    "     sp_codes.append(SP_Code)\n",
    "     sp_names.append(SP_Name)\n",
    "     probs.append(probabilities[0].numpy().tolist()[argmax])\n",
    "     file_names.append(os.path.splitext(os.path.basename(file_path))[0])\n",
    "     file_paths.append(file_path)\n",
    "     recording_durations.append(duration)\n",
    "     number_5s_increments.append(num_increments)\n",
    "\n",
    "# Create a dictionary to hold column data\n",
    "data = {\n",
    "    'file_name': file_names,\n",
    "    'file_path': file_paths,\n",
    "    'recording_duration': recording_durations,\n",
    "    'number_5s_increments': number_5s_increments,\n",
    "    'increment_number': increment_numbers,\n",
    "    'Sp_code': sp_codes,\n",
    "    'Sp_name': sp_names,\n",
    "    'Prob': probs\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdNET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
